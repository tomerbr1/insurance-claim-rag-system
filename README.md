# Insurance Claims RAG System

A multi-agent retrieval-augmented generation (RAG) system for insurance claim document analysis, implementing a **hybrid architecture** that combines SQL for structured queries with semantic search for unstructured narratives.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Key Design Decisions](#key-design-decisions)
- [Installation](#installation)
- [Usage](#usage)
- [Evaluation](#evaluation)
- [Project Structure](#project-structure)
- [Limitations & Trade-offs](#limitations--trade-offs)

---

## Overview

This system processes 10 insurance claim documents and provides intelligent query routing to answer both:
- **Structured queries**: "Show me all claims over $100k" â†’ SQL
- **Narrative queries**: "What happened in claim X?" â†’ RAG

### Features

âœ… **Hybrid Architecture** - SQL + RAG for optimal performance  
âœ… **3-Way Query Routing** - Intelligent classification to specialized agents  
âœ… **Hierarchical Chunking** - Multi-level chunks (128/512/1536 tokens)  
âœ… **Auto-Merging Retrieval** - Automatic context expansion  
âœ… **Multi-Level Summaries** - Chunk â†’ Section â†’ Document summaries  
âœ… **LLM-as-Judge Evaluation** - Automated quality assessment  
âœ… **MCP Integration** - ChromaDB vector store access  

---

## Architecture

### High-Level System Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Query    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Router Agent   â”‚ â—„â”€â”€ Classifies query type
â”‚  (3-way routing)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         â”‚            â”‚
    â–¼         â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚STRUCT â”‚ â”‚SUMMARYâ”‚ â”‚  NEEDLE  â”‚
â”‚ Agent â”‚ â”‚ Agent â”‚ â”‚  Agent   â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
    â”‚         â”‚          â”‚
    â–¼         â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚SQLite â”‚ â”‚Summaryâ”‚ â”‚Hierarchi-â”‚
â”‚  DB   â”‚ â”‚ Index â”‚ â”‚cal Index â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚         â”‚          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   Response   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Agent Routing Logic

| Query Type | Route To | Example |
|------------|----------|---------|
| Exact lookups | **Structured** | "Get claim CLM-2024-001847" |
| Filters & aggregations | **Structured** | "Claims over $50k", "Average value" |
| High-level overviews | **Summary** | "What happened in this claim?" |
| Timeline narratives | **Summary** | "Summarize the events" |
| Precise facts | **Needle** | "What was the exact towing cost?" |
| Reference IDs | **Needle** | "Wire transfer number?" |

### Data Flow

```
PDF Documents
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           DATA LOADING                      â”‚
â”‚  â€¢ Load PDFs with SimpleDirectoryReader    â”‚
â”‚  â€¢ Extract metadata using LLM (not regex)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚
        â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ METADATA STOREâ”‚  â”‚ HIERARCHICAL CHUNKING  â”‚
â”‚   (SQLite)    â”‚  â”‚  Small: 128 tokens     â”‚
â”‚               â”‚  â”‚  Medium: 512 tokens    â”‚
â”‚ â€¢ claim_id    â”‚  â”‚  Large: 1536 tokens    â”‚
â”‚ â€¢ claim_type  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ â€¢ total_value â”‚              â”‚
â”‚ â€¢ dates       â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚                 â”‚
                      â–¼                 â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  SUMMARY   â”‚    â”‚  VECTOR    â”‚
               â”‚   INDEX    â”‚    â”‚   INDEX    â”‚
               â”‚            â”‚    â”‚ (ChromaDB) â”‚
               â”‚ MapReduce: â”‚    â”‚            â”‚
               â”‚ â€¢ Chunk    â”‚    â”‚ Leaf nodes â”‚
               â”‚ â€¢ Section  â”‚    â”‚ only       â”‚
               â”‚ â€¢ Document â”‚    â”‚            â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Design Decisions

### 1. Hybrid Architecture: SQL + RAG

**Decision**: Use SQL for structured queries, RAG for narrative queries.

**Rationale**:
- Insurance claims contain both structured data (dates, IDs, amounts) and unstructured narratives
- SQL queries are **100x faster** for exact matches and filters
- No hallucination on structured data - SQL is deterministic
- RAG provides semantic understanding where SQL cannot

**Performance Comparison**:
| Query | Pure RAG | Hybrid |
|-------|----------|--------|
| "Get claim CLM-001847" | ~500ms | ~5ms |
| "Claims over $50k" | ~1000ms | ~10ms |
| "What happened?" | ~500ms | ~500ms |

### 2. LLM-Based Metadata Extraction (No Regex)

**Decision**: Use GPT-3.5-turbo to extract metadata from PDFs instead of regex patterns.

**Rationale**:
- **Format flexibility**: Handles date variations (Oct 15, 2024 / 2024-10-15 / 10/15/2024)
- **Semantic understanding**: Distinguishes incident_date from filing_date from settlement_date
- **Robustness**: No brittle regex patterns to maintain as document formats evolve
- **Consistency**: Aligns with RAG-first philosophy

**Trade-offs**:
- Higher cost (~$0.01 per document vs free regex)
- Slower (~2s per document vs instant)
- **BUT**: More reliable and production-ready

### 3. Multi-Level Summary Index (MapReduce)

**Decision**: Store intermediate summaries at chunk, section, and document levels.

**Rationale**:
- Different queries need different granularity
- "What were the repair costs?" â†’ Section summary (detailed)
- "Overview of claim" â†’ Document summary (high-level)
- Enables flexible retrieval at appropriate detail level

**Structure**:
```
Document CLM-2024-001847
â”œâ”€â”€ Document Summary (1 node)
â”‚   "Auto accident claim, Robert Mitchell, $14,050.33 settled..."
â”‚
â”œâ”€â”€ Section Summaries (3-5 nodes)
â”‚   â”œâ”€â”€ "Incident: Oct 15, 2024, intersection collision..."
â”‚   â”œâ”€â”€ "Costs: Towing $185, repairs $8,500..."
â”‚   â””â”€â”€ "Timeline: Filed Oct 16, settled Nov 30..."
â”‚
â””â”€â”€ Chunk Summaries (10-20 nodes)
    â”œâ”€â”€ "Police report by Officer Thompson, Badge #4421..."
    â”œâ”€â”€ "Tow Invoice #T-8827, $185.00..."
    â””â”€â”€ ...
```

### 4. Hierarchical Chunking with Auto-Merging

**Decision**: Use three chunk sizes (128/512/1536 tokens) with auto-merging retrieval.

**Rationale**:
- **Small chunks (128)**: Capture precise facts (amounts, dates, IDs)
- **Medium chunks (512)**: Provide context for timeline events
- **Large chunks (1536)**: Full sections for comprehensive understanding
- **Auto-merge**: Automatically expands context when needed

**Parameters**:
| Level | Tokens | Overlap | Purpose |
|-------|--------|---------|---------|
| Small | 128 | 20 | Precise facts |
| Medium | 512 | 50 | Balanced context |
| Large | 1536 | 200 | Full sections |

**Auto-Merge Threshold**: 50% - If more than half of a parent's children are retrieved, merge to parent.

### 5. MCP Integration

**Decision**: Integrate with ChromaDB MCP server rather than custom tools.

**Rationale**:
- Industry-standard protocol for LLM tool integration
- Community-maintained servers
- Richer functionality than custom tools
- More relevant to production systems

---

## Installation

### Prerequisites

- **Python 3.11.14** (required)
- **OpenAI API key** (required) - Get from https://platform.openai.com/api-keys
- **Google API key** (required) - Get from https://makersuite.google.com/app/apikey

**âš ï¸ Both API keys are required!**
- OpenAI: For main agents, routing, and processing
- Google: For unbiased evaluation (Gemini evaluates OpenAI outputs)

### Setup

```bash
# Clone or navigate to project
cd insurance-claim-rag-system

# Ensure you have Python 3.11.14 installed
python --version  # Should show Python 3.11.14

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp env.example .env
# Edit .env and add BOTH API keys:
#   OPENAI_API_KEY=sk-your-key-here
#   GOOGLE_API_KEY=your-key-here
```

**Important:** The system will validate both API keys at startup and fail fast if either is missing or invalid. This ensures unbiased evaluation throughout.

---

## Usage

### Interactive Mode

```bash
python main.py
```

Example queries:
```
ğŸ’¬ Get claim CLM-2024-001847
ğŸ”€ Routed to: STRUCTURED agent
ğŸ’¡ Auto Accident claim by Robert J. Mitchell, total $14,050.33, SETTLED

ğŸ’¬ What happened in the slip and fall claim?
ğŸ”€ Routed to: SUMMARY agent
ğŸ’¡ Patricia Vaughn suffered a slip and fall at Sunny Days Cafe...

ğŸ’¬ What was the exact towing cost in CLM-2024-001847?
ğŸ”€ Routed to: NEEDLE agent
ğŸ’¡ The towing cost was $185.00 (Tow Invoice #T-8827)
```

### Evaluation Mode

```bash
python main.py --eval
```

Runs 11 test cases and reports:
- Correctness score (0-1)
- Relevancy score (0-1)
- Recall score (0-1)
- Routing accuracy

### Build Only

```bash
python main.py --build
```

Builds indexes without entering interactive mode.

---

## Evaluation

### Methodology

We use **LLM-as-Judge** evaluation with a completely different provider (Gemini 2.5 Flash) evaluating responses from the main system (OpenAI GPT-4). This avoids potential bias from having OpenAI evaluate its own outputs.

### Metrics

| Metric | Description | Scoring |
|--------|-------------|---------|
| **Correctness** | Does answer match ground truth? | 0.0 - 1.0 |
| **Relevancy** | Was retrieved context relevant? | 0.0 - 1.0 |
| **Recall** | Were correct documents retrieved? | 0.0 - 1.0 |
| **Routing** | Did router choose correct agent? | Boolean |

### Test Cases

**Structured Queries (3)**:
- Exact lookups
- Range filters
- Status checks

**Summary Queries (2)**:
- Claim overviews
- Multi-claim summaries

**Needle Queries (6)**:
- Exact amounts
- Reference numbers
- Precise times
- Person names

### Sample Results

```
ğŸ“Š EVALUATION SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ˆ OVERALL SCORES:
   Correctness:      0.85
   Relevancy:        0.90
   Recall:           0.82
   Routing Accuracy: 91%

ğŸ“‹ BY QUERY TYPE:
   STRUCTURED:
      Avg Correctness: 0.93
      Routing Accuracy: 100%
   
   SUMMARY:
      Avg Correctness: 0.88
      Routing Accuracy: 100%
   
   NEEDLE:
      Avg Correctness: 0.80
      Routing Accuracy: 83%
```

---

## Project Structure

```
insurance-claim-rag-system/
â”œâ”€â”€ main.py                    # Entry point & orchestrator
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ env.example               # Environment template
â”œâ”€â”€ README.md                 # This file
â”‚
â”œâ”€â”€ insurance_claims_data/    # PDF claim documents
â”‚   â”œâ”€â”€ CLM_2024_001847.pdf
â”‚   â”œâ”€â”€ ... (10 PDFs)
â”‚   â””â”€â”€ README.md             # Data documentation
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py             # Configuration & settings
â”‚   â”œâ”€â”€ data_loader.py        # PDF loading + LLM metadata extraction
â”‚   â”œâ”€â”€ metadata_store.py     # SQLite for structured queries
â”‚   â”œâ”€â”€ chunking.py           # Hierarchical node parser
â”‚   â”œâ”€â”€ indexing.py           # Summary & vector index builders
â”‚   â”œâ”€â”€ retrieval.py          # Auto-merging retriever
â”‚   â”œâ”€â”€ evaluation.py         # LLM-as-judge evaluation
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ router_agent.py   # 3-way query router
â”‚   â”‚   â”œâ”€â”€ structured_agent.py # SQL-based queries
â”‚   â”‚   â”œâ”€â”€ summary_agent.py  # High-level RAG
â”‚   â”‚   â””â”€â”€ needle_agent.py   # Precise fact retrieval
â”‚   â”‚
â”‚   â””â”€â”€ mcp/
â”‚       â””â”€â”€ chromadb_client.py # MCP integration
â”‚
â””â”€â”€ tests/
    â””â”€â”€ test_queries.py       # Test query definitions
```

---

## Limitations & Trade-offs

### Known Limitations

1. **Cold Start**: Initial index building takes 2-3 minutes due to LLM summarization
2. **Cost**: LLM-based metadata extraction costs ~$0.10 per full build
3. **Context Window**: Very large claims may exceed context limits
4. **Routing Accuracy**: Edge cases between summary/needle can misroute

### Trade-offs Made

| Decision | Benefit | Cost |
|----------|---------|------|
| LLM for metadata | Robust extraction | Higher cost, slower |
| Store all summaries | Flexible retrieval | More storage |
| 3-way routing | Better accuracy | More complexity |
| Small chunks (128) | Precise retrieval | May lose context |

### Future Improvements

- [ ] Add caching for LLM calls
- [ ] Implement streaming responses
- [ ] Add document section detection for better chunking
- [ ] Support multi-modal (images in PDFs)
- [ ] Add conversation history for follow-up queries

---

## Models Used

| Component | Model | Provider | Purpose |
|-----------|-------|----------|---------|
| **Agents & Routing** | GPT-4 | OpenAI | Query processing, response generation |
| **Metadata Extraction** | GPT-4o-mini | OpenAI | Extract structured data from PDFs (cost-effective) |
| **Embeddings** | text-embedding-3-small | OpenAI | Vector representations |
| **Evaluation Judge** | Gemini 2.5 Flash | Google | Score system responses (different provider for unbiased evaluation) |

---

## License

This project is part of an academic assignment.

---

## Author

Insurance Claims RAG System - Tomer Brami

