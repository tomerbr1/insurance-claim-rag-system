# Insurance Claims RAG System

A multi-agent retrieval-augmented generation (RAG) system for insurance claim document analysis, implementing a **hybrid architecture** that combines SQL for structured queries with semantic search for unstructured narratives.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Key Design Decisions](#key-design-decisions)
- [Installation](#installation)
- [Usage](#usage)
- [Evaluation](#evaluation)
- [Project Structure](#project-structure)
- [Limitations & Trade-offs](#limitations--trade-offs)

---

## Overview

This system processes 13 insurance claim documents and provides intelligent query routing to answer both:
- **Structured queries**: "Show me all claims over $100k" â†’ SQL
- **Narrative queries**: "What happened in claim X?" â†’ RAG

### Features

- **Hybrid Architecture** - SQL + RAG for optimal performance
- **3-Way Query Routing** - Intelligent classification to specialized agents
- **Hierarchical Chunking** - 2-level chunks (1024/256 tokens)
- **Auto-Merging Retrieval** - Automatic context expansion
- **Multi-Level Summaries** - Chunk â†’ Section â†’ Document summaries
- **Table Extraction** - Automatic table detection and markdown conversion
- **LLM-as-Judge Evaluation** - Automated quality assessment
- **MCP Integration** - ChromaDB vector store access  

---

## Architecture

![System Architecture](assets/architecture_diagram.jpg)

### High-Level System Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Query    â”‚
â”‚ "What was the   â”‚
â”‚  towing cost?"  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Router Agent (GPT-4)       â”‚
â”‚  Classifies query into:         â”‚
â”‚  â€¢ STRUCTURED (SQL metadata)    â”‚
â”‚  â€¢ SUMMARY (high-level overview)â”‚
â”‚  â€¢ NEEDLE (precise fact)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         â”‚            â”‚
    â–¼         â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚STRUCT â”‚ â”‚SUMMARYâ”‚ â”‚  NEEDLE  â”‚
â”‚ Agent â”‚ â”‚ Agent â”‚ â”‚  Agent   â”‚
â”‚(GPT-4)â”‚ â”‚(GPT-4)â”‚ â”‚ (GPT-4)  â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
    â”‚         â”‚          â”‚
    â”‚         â”‚          â”‚ Auto-merging
    â–¼         â–¼          â–¼ retrieval
â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚SQLite â”‚ â”‚Summaryâ”‚ â”‚Hierarchi-â”‚
â”‚  DB   â”‚ â”‚ Index â”‚ â”‚cal Index â”‚
â”‚       â”‚ â”‚       â”‚ â”‚(ChromaDB)â”‚
â”‚10 rowsâ”‚ â”‚~200   â”‚ â”‚~150 leaf â”‚
â”‚       â”‚ â”‚nodes  â”‚ â”‚nodes     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚         â”‚          â”‚
    â”‚         â”‚          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   Response   â”‚
      â”‚ + Citations  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Agent Routing Logic

| Query Type | Route To | Example |
|------------|----------|---------|
| Exact lookups | **Structured** | "Get claim CLM-2024-001847" |
| Filters & aggregations | **Structured** | "Claims over $50k", "Average value" |
| High-level overviews | **Summary** | "What happened in this claim?" |
| Timeline narratives | **Summary** | "Summarize the events" |
| Precise facts | **Needle** | "What was the exact towing cost?" |
| Reference IDs | **Needle** | "Wire transfer number?" |

### Data Flow

```
PDF Documents (13 claims, 3 with tables)
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           DATA LOADING                      â”‚
â”‚  â€¢ Load PDFs with SimpleDirectoryReader    â”‚
â”‚  â€¢ Extract tables using pdfplumber         â”‚
â”‚  â€¢ Convert tables to markdown format       â”‚
â”‚  â€¢ Extract metadata using LLM (GPT-4o-mini)â”‚
â”‚    (not regex - handles format variations) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚
        â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ METADATA STOREâ”‚  â”‚ HIERARCHICAL CHUNKING  â”‚
â”‚   (SQLite)    â”‚  â”‚  2 levels with overlap â”‚
â”‚               â”‚  â”‚                        â”‚
â”‚ â€¢ claim_id    â”‚  â”‚  Large:  1024 tokens   â”‚
â”‚ â€¢ claim_type  â”‚  â”‚    â””â”€ Small: 256       â”‚
â”‚ â€¢ total_value â”‚  â”‚                        â”‚
â”‚ â€¢ dates, etc. â”‚  â”‚  Overlap: 20 tokens    â”‚
â”‚               â”‚  â”‚                        â”‚
â”‚ Used by:      â”‚  â”‚  Creates parent-child  â”‚
â”‚ Structured    â”‚  â”‚  relationships in      â”‚
â”‚ Agent         â”‚  â”‚  DocumentStore         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚                 â”‚
                      â–¼                 â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  SUMMARY   â”‚    â”‚  VECTOR    â”‚
               â”‚   INDEX    â”‚    â”‚   INDEX    â”‚
               â”‚            â”‚    â”‚ (ChromaDB) â”‚
               â”‚ MapReduce: â”‚    â”‚            â”‚
               â”‚ â€¢ Chunk    â”‚    â”‚ Leaf nodes â”‚
               â”‚ â€¢ Section  â”‚    â”‚ only       â”‚
               â”‚ â€¢ Document â”‚    â”‚            â”‚
               â”‚            â”‚    â”‚ Auto-      â”‚
               â”‚ ~200 nodes â”‚    â”‚ merging    â”‚
               â”‚            â”‚    â”‚ retrieval  â”‚
               â”‚ Used by:   â”‚    â”‚            â”‚
               â”‚ Summary    â”‚    â”‚ Used by:   â”‚
               â”‚ Agent      â”‚    â”‚ Needle     â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ Agent      â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚ Router Agent   â”‚
                      â”‚ (Coordinates   â”‚
                      â”‚  all 3 agents) â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Processing Pipeline:**

| Phase | Component | Output | Notes |
|-------|-----------|--------|-------|
| 1. Load | `SimpleDirectoryReader` + pdfplumber | 13 Documents | Raw PDF text + tables |
| 2. Tables | pdfplumber | Markdown tables | Appended to text |
| 3. Extract | LLM (GPT-4o-mini) | Metadata dicts | ~$0.01 cost |
| 4. Store | SQLite + Chunking | DB + Nodes | Dual path |
| 5. Summarize | MapReduce (GPT-4) | Summary nodes | 3 levels |
| 6. Embed | ChromaDB + OpenAI | Vector index | Leaf nodes only |
| 7. Build | Agent creation | 4 agents | Router + 3 specialists |
| 8. Query | Runtime routing | Responses | Per-query execution |

**Total build time:** ~2-3 minutes | **Total cost:** ~$0.10-0.15

---

### Data Processing Pipeline Details

The system processes data through a carefully orchestrated pipeline:

**Pipeline Execution Time:** ~2-3 minutes for 13 documents

**Phase-by-Phase Breakdown:**

| Phase | Time | LLM Calls | Storage | Notes |
|-------|------|-----------|---------|-------|
| **1. Load PDFs** | ~5s | 0 | Memory | 13 PDFs â†’ Document objects |
| **2. Extract Tables** | ~2s | 0 | Memory | pdfplumber â†’ markdown |
| **3. Extract Metadata** | ~25s | 13 | SQLite | GPT-4o-mini, 1 call/doc |
| **4. Chunk Documents** | ~10s | 0 | Memory + DocStore | 2 levels, ~250-350 nodes |
| **5. Build Summary Index** | ~90-120s | ~180-220 | VectorStore | Most expensive phase |
| **6. Build Vector Index** | ~30-45s | ~180 | ChromaDB | Embedding generation |
| **7. Create Agents** | ~1s | 0 | Memory | Configure 4 agents |
| **8. Ready for Queries** | Instant | 1-2/query | - | Runtime queries |

**Storage Footprint:**
- **SQLite database**: ~65 KB (13 rows of structured metadata)
- **ChromaDB vector store**: ~6-12 MB (embeddings + metadata)
- **In-memory objects**: ~15-20 MB (document store, nodes, agents)

**Cost Breakdown (per full rebuild):**
- Metadata extraction (GPT-4o-mini): ~$0.01-0.02
- Summary generation (GPT-4): ~$0.08-0.12
- Embedding generation (text-embedding-3-small): ~$0.01
- **Total per rebuild**: ~$0.10-0.15

**Runtime Query Costs:**
- Structured query: ~$0.001 (minimal LLM use)
- Summary query: ~$0.005-0.01 (routing + generation)
- Needle query: ~$0.01-0.02 (routing + retrieval + generation)

---

## Key Design Decisions

### 1. Hybrid Architecture: SQL + RAG

**Decision**: Use SQL for structured queries, RAG for narrative queries.

**Rationale**:
- Insurance claims contain both structured data (dates, IDs, amounts) and unstructured narratives
- SQL queries are **100x faster** for exact matches and filters
- No hallucination on structured data - SQL is deterministic
- RAG provides semantic understanding where SQL cannot

**Performance Comparison**:
| Query | Pure RAG | Hybrid |
|-------|----------|--------|
| "Get claim CLM-001847" | ~500ms | ~5ms |
| "Claims over $50k" | ~1000ms | ~10ms |
| "What happened?" | ~500ms | ~500ms |

### 2. LLM-Based Metadata Extraction (No Regex)

**Decision**: Use GPT-3.5-turbo to extract metadata from PDFs instead of regex patterns.

**Rationale**:
- **Format flexibility**: Handles date variations (Oct 15, 2024 / 2024-10-15 / 10/15/2024)
- **Semantic understanding**: Distinguishes incident_date from filing_date from settlement_date
- **Robustness**: No brittle regex patterns to maintain as document formats evolve
- **Consistency**: Aligns with RAG-first philosophy

**Trade-offs**:
- Higher cost (~$0.01 per document vs free regex)
- Slower (~2s per document vs instant)
- **BUT**: More reliable and production-ready

### 3. Multi-Level Summary Index (MapReduce)

**Decision**: Store intermediate summaries at chunk, section, and document levels.

**Rationale**:
- Different queries need different granularity
- "What were the repair costs?" â†’ Section summary (detailed)
- "Overview of claim" â†’ Document summary (high-level)
- Enables flexible retrieval at appropriate detail level

**MapReduce Implementation**:

The system uses a true MapReduce pattern with LLM calls:

**MAP Phase** (Per-chunk summarization):
- Input: ~10-20 small chunks per claim
- Process: Each chunk â†’ GPT-4 â†’ 2-3 sentence summary
- Output: Chunk-level summary nodes (stored as retrievable TextNodes)
- LLM calls: ~100-200 total (10-20 per claim Ã— 10 claims)

**REDUCE Phase 1** (Section-level aggregation):
- Input: All chunk summaries for a claim
- Process: Combine summaries â†’ GPT-4 â†’ 3-5 sentence section summary
- Output: Section-level summary nodes
- LLM calls: ~10 (1 per claim)

**REDUCE Phase 2** (Document-level aggregation):
- Input: Section summaries for a claim
- Process: Create comprehensive summary â†’ GPT-4 â†’ 1 paragraph
- Output: Document-level summary node
- LLM calls: ~10 (1 per claim)

**Storage Structure**:
```
Document CLM-2024-001847
â”œâ”€â”€ Document Summary (1 node) â† stored in VectorStoreIndex
â”‚   metadata: {'summary_level': 'document', 'claim_id': 'CLM-2024-001847'}
â”‚   text: "Auto accident claim by Robert Mitchell, total $14,050.33, 
â”‚          settled Nov 30, 2024. Intersection collision, vehicle towed..."
â”‚
â”œâ”€â”€ Section Summary (1 node) â† stored in VectorStoreIndex
â”‚   metadata: {'summary_level': 'section', 'claim_id': 'CLM-2024-001847'}
â”‚   text: "Incident occurred Oct 15, 2024 at Main St intersection.
â”‚          Towing: $185 (Invoice #T-8827). Repairs: $8,500..."
â”‚
â””â”€â”€ Chunk Summaries (15 nodes) â† stored in VectorStoreIndex
    â”œâ”€â”€ metadata: {'summary_level': 'chunk', 'chunk_index': 0, ...}
    â”‚   text: "Police report by Officer Thompson, Badge #4421..."
    â”œâ”€â”€ metadata: {'summary_level': 'chunk', 'chunk_index': 1, ...}
    â”‚   text: "Tow Invoice #T-8827, dated Oct 15, amount $185.00..."
    â””â”€â”€ ...
```

**All summary levels are vectorized and retrievable**, enabling the Summary Agent to find the most appropriate granularity based on the query.

### 4. Hierarchical Chunking with Auto-Merging

**Decision**: Use two chunk sizes (1024/256 tokens) with auto-merging retrieval.

**Rationale**:
- **Small chunks (256)**: Capture precise facts (amounts, dates, IDs)
- **Large chunks (1024)**: Full sections for comprehensive understanding
- **Auto-merge**: Automatically expands context when needed
- Two levels are sufficient for insurance claims documents

**Chunking Parameters**:
| Level | Tokens | Overlap | Purpose | Indexed? |
|-------|--------|---------|---------|----------|
| Small | 256 | 20 | Precise facts | âœ… Yes (in ChromaDB) |
| Large | 1024 | 20 | Full sections | âŒ No (in DocStore only) |

**How Auto-Merging Works:**

1. **Initial Retrieval**: 
   - Query â†’ ChromaDB vector search â†’ Top-k small chunks (k=5)
   - Example: Query "towing cost" retrieves 5 small chunks about towing

2. **Merge Decision**:
   - For each retrieved small chunk, check its siblings (other children of same parent)
   - If >50% of siblings are also retrieved â†’ merge to parent (medium chunk)
   - Example: 3 out of 5 small chunks share same medium parent â†’ merge to medium

3. **Recursive Merging**:
   - Apply same logic to medium chunks
   - Can merge medium â†’ large if threshold met
   - Example: If multiple medium chunks from same section retrieved â†’ expand to large

4. **Context Expansion**:
   - User gets expanded context automatically
   - No need to manually adjust chunk size
   - Balances precision with context

**Auto-Merge Threshold**: 50% (configurable in `AutoMergingRetriever`)

**Example Retrieval Flow**:
```
Query: "What was the exact towing cost in CLM-2024-001847?"

Step 1 - Initial retrieval:
  â†’ Retrieved 5 small chunks (256 tokens each)
  â†’ chunk_0: "Tow Invoice #T-8827..."
  â†’ chunk_1: "Amount: $185.00..."
  â†’ chunk_2: "Date: Oct 15, 2024..."
  â†’ chunk_8: "Other claim detail..."
  â†’ chunk_15: "Another detail..."

Step 2 - Check merge conditions:
  â†’ chunk_0, chunk_1, chunk_2 share parent: large_0
  â†’ 3 out of 4 children retrieved (75% > 50% threshold)
  â†’ MERGE to large_0

Step 3 - Return expanded context:
  â†’ Original: 768 tokens (3 Ã— 256)
  â†’ After merge: 1024 tokens (1 large chunk)
  â†’ Includes full section context about towing incident
```

This ensures precise retrieval with automatic context expansion when needed.

### 5. MCP Integration

**Decision**: Integrate with ChromaDB MCP server rather than custom tools.

**Rationale**:
- Industry-standard protocol for LLM tool integration
- Community-maintained servers
- Richer functionality than custom tools
- More relevant to production systems

**Implementation**:

The system uses an MCP-style wrapper (`ChromaDBMCPClient`) that provides:

| Tool | Description | Used By |
|------|-------------|---------|
| `list_collections` | List all vector store collections | Router Agent |
| `collection_stats` | Get collection document count | Router Agent |
| `collection_count` | Quick document count | Router Agent |
| `direct_search` | Semantic similarity search | Available |
| `peek_collection` | Preview collection documents | Available |

**Router Agent MCP Integration**:

The Router Agent calls MCP tools before making routing decisions, demonstrating agent-tool interaction:

```
ğŸ’¬ Query: What was the towing cost?

ğŸ”§ [MCP TOOL CALL] collection_stats()
   â””â”€ Result: Collection 'insurance_claims': 150 documents
ğŸ”§ [MCP TOOL CALL] collection_stats('insurance_claims_summaries')
   â””â”€ Result: Collection 'insurance_claims_summaries': 200 documents

ğŸ”€ Routed to: NEEDLE agent
ğŸ’¡ Answer: The towing cost was $185.00 (Tow Invoice #T-8827)
```

This visible tool call output demonstrates the MCP integration in action.

**Interactive MCP Status**:

Use the `mcp` command in interactive mode to view MCP client status and test tools.

### 6. Table Extraction (pdfplumber)

**Decision**: Extract tables from PDFs using pdfplumber and convert to markdown for LLM consumption.

**Rationale**:
- Insurance documents often contain tabular data (financial breakdowns, coverage limits, treatment timelines)
- Standard PDF text extraction (pypdf) loses table structure, making data hard to interpret
- pdfplumber preserves table structure as 2D arrays
- Markdown format is well-understood by LLMs and maintains visual structure

**Implementation**:

The system uses a dual-extraction approach:
1. **Base text**: pypdf extracts narrative content
2. **Tables**: pdfplumber detects and extracts tables as 2D arrays
3. **Conversion**: Tables are converted to markdown format
4. **Integration**: Tables are appended to document text with markers

**Table-Enabled Documents**:

| Claim ID | Table Type | Key Data |
|----------|------------|----------|
| CLM-2024-006001 | Financial Breakdown | 8 line items, costs, dates |
| CLM-2024-006002 | Coverage/Policy | 7 coverage types with limits |
| CLM-2024-006003 | Treatment Timeline | 12 events with status |

**Example Extracted Table**:
```
[EXTRACTED TABLE - Page 1]
| Item             | Category   | Amount     | Date       |
|------------------|------------|------------|------------|
| Roof replacement | Structural | $12,500.00 | 2024-09-15 |
| Water cleanup    | Restoration| $3,200.00  | 2024-09-12 |
[END TABLE]
```

**Configuration**:
- `EXTRACT_TABLES=true` - Enable/disable table extraction
- `TABLE_MIN_ROWS=2` - Minimum rows for valid table
- `TABLE_MIN_COLS=2` - Minimum columns for valid table

**Test Queries for Tables**:
- "What was the exact cost of roof replacement?" â†’ $12,500.00
- "What is the Medical Payments coverage limit?" â†’ $5,000
- "How much did the MRI cost?" â†’ $2,800.00

---

## Installation

### Prerequisites

- **Python 3.11.14** (required)
- **OpenAI API key** (required) - Get from https://platform.openai.com/api-keys
- **Google API key** (required) - Get from https://makersuite.google.com/app/apikey

**âš ï¸ Both API keys are required!**
- OpenAI: For main agents, routing, and processing
- Google: For unbiased evaluation (Gemini evaluates OpenAI outputs)

### Setup

```bash
# Clone or navigate to project
cd insurance-claim-rag-system

# Ensure you have Python 3.11.14 installed
python --version  # Should show Python 3.11.14

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp env.example .env
# Edit .env and add BOTH API keys:
#   OPENAI_API_KEY=sk-your-key-here
#   GOOGLE_API_KEY=your-key-here
```

**Important:** The system will validate both API keys at startup and fail fast if either is missing or invalid. This ensures unbiased evaluation throughout.

---

## Usage

### Interactive Mode

```bash
python main.py
```

Example queries:
```
ğŸ’¬ Get claim CLM-2024-001847
ğŸ”€ Routed to: STRUCTURED agent
ğŸ’¡ Auto Accident claim by Robert J. Mitchell, total $14,050.33, SETTLED

ğŸ’¬ What happened in the slip and fall claim?
ğŸ”€ Routed to: SUMMARY agent
ğŸ’¡ Patricia Vaughn suffered a slip and fall at Sunny Days Cafe...

ğŸ’¬ What was the exact towing cost in CLM-2024-001847?
ğŸ”€ Routed to: NEEDLE agent
ğŸ’¡ The towing cost was $185.00 (Tow Invoice #T-8827)
```

### Evaluation Mode

```bash
python main.py --eval
```

Runs 11 test cases and reports:
- Correctness score (0-1)
- Relevancy score (0-1)
- Recall score (0-1)
- Routing accuracy

### Build Only

```bash
python main.py --build
```

Builds indexes without entering interactive mode.

---

## Evaluation

### Methodology

We use **LLM-as-Judge** evaluation with a completely different provider (Gemini 2.5 Flash) evaluating responses from the main system (OpenAI GPT-4). This avoids potential bias from having OpenAI evaluate its own outputs.

### Metrics

| Metric | Description | Scoring |
|--------|-------------|---------|
| **Correctness** | Does answer match ground truth? | 0.0 - 1.0 |
| **Relevancy** | Was retrieved context relevant? | 0.0 - 1.0 |
| **Recall** | Were correct documents retrieved? | 0.0 - 1.0 |
| **Routing** | Did router choose correct agent? | Boolean |

### Test Cases

**Structured Queries (3)**:
- Exact lookups
- Range filters
- Status checks

**Summary Queries (2)**:
- Claim overviews
- Multi-claim summaries

**Needle Queries (6)**:
- Exact amounts
- Reference numbers
- Precise times
- Person names

### Sample Results

```
ğŸ“Š EVALUATION SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ˆ OVERALL SCORES:
   Correctness:      0.85
   Relevancy:        0.90
   Recall:           0.82
   Routing Accuracy: 91%

ğŸ“‹ BY QUERY TYPE:
   STRUCTURED:
      Avg Correctness: 0.93
      Routing Accuracy: 100%
   
   SUMMARY:
      Avg Correctness: 0.88
      Routing Accuracy: 100%
   
   NEEDLE:
      Avg Correctness: 0.80
      Routing Accuracy: 83%
```

---

## Project Structure

```
insurance-claim-rag-system/
â”œâ”€â”€ main.py                    # Entry point & orchestrator
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ env.example               # Environment template
â”œâ”€â”€ README.md                 # This file
â”‚
â”œâ”€â”€ insurance_claims_data/    # PDF claim documents
â”‚   â”œâ”€â”€ CLM_2024_001847.pdf
â”‚   â”œâ”€â”€ ... (13 PDFs total, 3 with tables)
â”‚   â””â”€â”€ README.md             # Data documentation
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ generate_table_pdfs.py # Generate PDFs with embedded tables
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py             # Configuration & settings
â”‚   â”œâ”€â”€ data_loader.py        # PDF loading + table extraction + LLM metadata
â”‚   â”œâ”€â”€ metadata_store.py     # SQLite for structured queries
â”‚   â”œâ”€â”€ chunking.py           # Hierarchical node parser
â”‚   â”œâ”€â”€ indexing.py           # Summary & vector index builders
â”‚   â”œâ”€â”€ retrieval.py          # Auto-merging retriever
â”‚   â”œâ”€â”€ evaluation.py         # LLM-as-judge evaluation
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ router_agent.py   # 3-way query router
â”‚   â”‚   â”œâ”€â”€ structured_agent.py # SQL-based queries
â”‚   â”‚   â”œâ”€â”€ summary_agent.py  # High-level RAG
â”‚   â”‚   â””â”€â”€ needle_agent.py   # Precise fact retrieval
â”‚   â”‚
â”‚   â””â”€â”€ mcp/
â”‚       â””â”€â”€ chromadb_client.py # MCP integration
â”‚
â””â”€â”€ tests/
    â””â”€â”€ test_queries.py       # Test query definitions
```

---

## Limitations & Trade-offs

### Known Limitations

1. **Cold Start**: Initial index building takes 2-3 minutes due to LLM summarization
2. **Cost**: LLM-based metadata extraction costs ~$0.10 per full build
3. **Context Window**: Very large claims may exceed context limits
4. **Routing Accuracy**: Edge cases between summary/needle can misroute

### Trade-offs Made

| Decision | Benefit | Cost |
|----------|---------|------|
| LLM for metadata | Robust extraction | Higher cost, slower |
| Store all summaries | Flexible retrieval | More storage |
| 3-way routing | Better accuracy | More complexity |
| Small chunks (256) | Precise retrieval | May lose context |

### Future Improvements

- [ ] Add caching for LLM calls
- [ ] Implement streaming responses
- [ ] Add document section detection for better chunking
- [ ] Support multi-modal (images in PDFs)
- [ ] Add conversation history for follow-up queries

---

## Models Used

| Component | Model | Provider | Purpose |
|-----------|-------|----------|---------|
| **Agents & Routing** | GPT-4 | OpenAI | Query processing, response generation |
| **Metadata Extraction** | GPT-4o-mini | OpenAI | Extract structured data from PDFs (cost-effective) |
| **Embeddings** | text-embedding-3-small | OpenAI | Vector representations |
| **Evaluation Judge** | Gemini 2.5 Flash | Google | Score system responses (different provider for unbiased evaluation) |

---

## License

This project is part of an academic assignment.

---

## Author

Insurance Claims RAG System - Tomer Brami

